# Backlog

## Feature Groups

### Feature 003-agent-intercom-release

- Rename solution to agent-intercom; remove ALL reference to the word monocoque.
- Root folder in project workspace should use .intercom
- MCP tool should show as "intercom" in list of tools in IDE
- Full product documentation update including setup guide, user guide, and developer guide. The existing README is a start but needs expansion and restructuring to cover all core features, usage patterns, and development workflows in a clear and organized way. Should also include full documentation on the -ctl.exe local admin utility.
- Release pipeline that produces release executable with feature flagging, tagging, gating, and other related capabilities.
- Not currently getting notifications to Slack of Approval requests, e.g. read a file outside the current workspace.
- Not currently getting notifications to Slack of diff_acceptance approvals.
- Not currently getting notifications to Slack of agent session continuation approvals.
- Upgrade rmcp crate to 0.13.0; breaking changes will require a full feature refactor to implement.
- Consider tool command names like "buzz", "base station", "substation", "half-duplex", etc that are "intercom" related terminology instead of the existing generic tool names.

### Feature 004-intercom-advanced-features

- When the HTTP bind fails (port 10048), the server doesn't exit. It fails the HTTP transport but the Slack Socket Mode connection succeeds independently, keeping the process alive as a zombie. The server should exit if the HTTP transport fails to bind, since it's a critical failure that prevents any MCP communication.  Also, the server should never allow more than one instance of itself to be spawnable at a time on the same machine, since that would cause conflicts and unpredictable behavior. **Fix:** In `src/main.rs`, if `transport::serve_http()` returns an error (e.g., due to bind failure), log the error and call `std::process::exit(1)` to terminate the process immediately. Additionally, implement a startup check that attempts to bind to the configured HTTP port before starting any services; if the bind fails, log a clear error message about another instance running and exit.
- Ability to configure different detail levels of context sharing/messaging to Slack channel user context sharing.
- Agent hang/failure should get reported to Slack with details and recommended next steps.
- Add audit log feature to .intercom/logs/ with structured log entries for all agent interactions, including tool calls, approvals, and session lifecycle events.
- Ability to log terminal command approvals and rejections with details of the command, timestamp, and operator identity to the audit log.
- Ability to ask the user if they want to add the approved commands to the local workspace policy for auto-approval in the future in .intercom/settings.json and applying RegEx to the command(s) to ensure the list is efficient.
- Enhance forward_prompt command: Both "Resume with Instructions" and "Refine" currently use placeholder strings ("(instruction via Slack)"). Slack modal support for collecting actual typed instructions is noted as future work in the handlers.
- Slack modal instruction capture for `wait_for_instruction` and `forward_prompt` (confirmed failing via HITL Scenario 7 retry): Both tools return `"(instruction via Slack)"` placeholder instead of the operator's actual typed text. **Root cause:** `wait_resume_instruct` (and `prompt_refine`) button handlers immediately resolve the oneshot with a hardcoded string; no text input is ever collected. **Fix requires a 3-step Slack modal flow:** (1) When `wait_resume_instruct` or `prompt_refine` is pressed, extract the `trigger_id` from the `BlockActions` payload and call `SlackService::open_modal()` (already exists in `src/slack/client.rs`) with a plain-text input block; store `session_id` in the modal's `private_metadata`; do NOT resolve the oneshot yet. (2) Add a `ViewSubmission` match arm to `src/slack/events.rs` (currently falls through to `"unhandled interaction event type"`); extract typed text and `session_id` from `private_metadata`. (3) New modal submit handler resolves the pending oneshot with the real instruction text. **Files to change:** `src/slack/handlers/wait.rs`, `src/slack/handlers/prompt.rs`, `src/slack/events.rs`, `src/slack/blocks.rs` (add modal view builder), `src/slack/handlers/mod.rs`. The `trigger_id` must be threaded from the `BlockActions` event down into the action handlers (currently not passed). This is the canonical fix for the placeholder string issue noted in the existing backlog item above.
- Additions to .intercom/settings.json should hot-reload to the server memory. `PolicyWatcher` already supports `register()` / `get_policy()` / `cache()` — the remaining work is wiring `PolicyCache` into `AppState` (cascades to ~11 struct constructions across 7 test files) and switching `check_auto_approve` from `PolicyLoader::load()` to cache reads. This should be a dedicated feature spec.
- SSE disconnect session cleanup: When an SSE connection drops (window reload, VS Code restart, network hiccup), the server should detect the stream closure and mark the corresponding session as `Terminated` or `Interrupted`. Currently, disconnected sessions remain `Active` indefinitely. Implementation likely involves hooking into the `SseServer` or axum stream lifecycle in `src/mcp/sse.rs` to trigger `session_repo.set_terminated()` on stream close.
- **[Low priority] Slack queue drain race during graceful shutdown when no global channel is configured**: In `src/main.rs::shutdown_with_timeout`, the graceful shutdown sequence is: (1) call `graceful_shutdown()`, which includes a 500ms sleep to let the Slack message queue drain — but *only* when `config.slack.channel_id` is non-empty; (2) abort `SlackRuntime.queue_task`. When the global channel is empty (i.e., all channel routing is per-workspace via SSE `?channel_id=` parameters), the 500ms drain sleep in step 1 is skipped. If any code path enqueues a Slack message between the time the shutdown signal fires and `queue_task.abort()` is called, those messages will be dropped. In practice this is unlikely because: (a) an empty global channel means no server-level shutdown/recovery notifications are posted; (b) per-session messages require a live MCP connection, which stops accepting new work when `CancellationToken` is cancelled before the sleep would run. **If this ever needs fixing**: move the queue drain sleep and the abort into `shutdown_with_timeout` itself (after all task handles are awaited), unconditionally — or add a `drain()` method to `SlackService` that flushes the channel before aborting the worker task. (Deferred from PR `001-002-integration-test` RI-007.)
- **Policy command regex pre-compilation via `regex::RegexSet`**: `src/policy/evaluator.rs::match_command_pattern` currently calls `Regex::new(pattern)` for every pattern on every `check_auto_approve` tool invocation, compiling regexes from scratch each call. **Fix:** Replace the per-call compilation loop with `regex::RegexSet`. Two approach options — (A) *Eager compile in loader* (preferred): `PolicyLoader::load()` returns a `CompiledWorkspacePolicy { raw: WorkspacePolicy, command_set: RegexSet }` and `PolicyEvaluator::evaluate()` takes `&CompiledWorkspacePolicy`; (B) *Lazy compile*: add `compiled_commands: OnceLock<RegexSet>` to `WorkspacePolicy`. Option A is cleaner (pure data struct, no `OnceLock`). **`RegexSet` replacement for `match_command_pattern`:** `let set = RegexSet::new(patterns)?;` then `set.matches(command).into_iter().next().map(|i| format!("command:{}", patterns[i]))` — one pass through all patterns, returns first matching index. **Files to change:** `src/models/policy.rs` (add `CompiledWorkspacePolicy` if Option A), `src/policy/loader.rs` (`load()` returns compiled form), `src/policy/evaluator.rs` (signature + inner function), `src/mcp/tools/check_auto_approve.rs` (use compiled policy type), `src/policy/watcher.rs` (cache stores compiled form). **Tests to update:** `tests/unit/policy_evaluator_tests.rs`, `tests/unit/policy_tests.rs`. `regex` crate already in `Cargo.toml` — no new dependency needed. (Deferred from PR `001-002-integration-test` RI-001.)
- Heartbeat fallback to most-recent session: When `heartbeat` finds multiple active sessions (e.g., a stale session that wasn't cleaned up plus a spawned session), instead of returning an ambiguity error, fall back to the session with the most recent `updated_at` timestamp. This would make `heartbeat` more resilient to edge cases where stale cleanup didn't run or a race condition created multiple active sessions. Requires changing the `list_active` → match logic in `src/mcp/tools/heartbeat.rs` to sort by `updated_at DESC` and pick the first.
- Operator steering queue: A shared-queue feature enabling proactive operator-to-agent communication without requiring the agent to block. **Core:** A `steering_message` DB table (columns: `id`, `session_id`, `message`, `source`, `created_at`, `consumed`) stores messages from any ingestion path. **Delivery:** The `heartbeat` response includes an optional `pending_steering: Vec<String>` array of unconsumed messages for the session, then marks them consumed — zero additional agent tool calls needed. **Ingestion paths (two):** (1) *Slack* — free-text `AppMention` messages or a new `/intercom steer <text>` slash command write to the queue; (2) *IPC/local* — a new `steer` command added to the IPC protocol (`src/ipc/server.rs`) and a `Steer { message }` subcommand in `intercom-ctl` (`ctl/main.rs`) allows a local PowerShell script (`intercom-ctl steer "refocus on error handling"`) to push a message without the agent being blocked. Both paths write to the same queue; `heartbeat` is the single delivery mechanism. This supersedes the existing first backlog item ("Ability to proactively engage an agent in active session") and is related to the `wait_for_instruction` placeholder string issue (see forward_prompt item).
- Task inbox (agent cold-start queue): A persistent `task_inbox` DB table (columns: `id`, `message`, `source`, `created_at`, `consumed`) that stores work items queued while no agent session is active. **Ingestion paths** mirror the steering queue: (1) *Slack* — a new `/intercom task <text>` slash command writes to the inbox; (2) *IPC/local* — a new `task` IPC command and `intercom-ctl task <text>` subcommand allow queuing work from a local script without the operator being present. **Delivery:** The agent reads and clears unconsumed inbox items at session startup — either surfaced in the `recover_state` response or via a dedicated `check_inbox` MCP tool call triggered early in `on_initialized`. **Distinction from steering queue:** The steering queue targets an already-running agent session (delivery via `heartbeat`); the task inbox targets a not-yet-started agent (delivery at cold-start). Complements `/intercom spawn` (which launches a child process with a prompt inline) by providing a queue-based alternative that doesn't require the operator to be present at spawn time — items accumulate and are drained when the next agent session starts.
- Agent heartbeat loop (keep-alive pattern): A documented agent-side pattern — not a server feature — that keeps a session alive and responsive without human interaction. The loop: call `heartbeat` → process any `pending_steering` messages from the operator steering queue → call `wait_for_instruction` with a timeout (e.g. 60 seconds) → on timeout, loop again; on instruction, act then loop. This is the most robust way to make a running session self-sustaining, and pairs directly with the operator steering queue (which delivers messages on each `heartbeat` wakeup). The pattern should be documented in the user guide and codified as a reusable agent prompt snippet (e.g. a `.github/prompts/heartbeat-loop.prompt.md`) so operators can easily instruct agents to enter keep-alive mode. No server-side changes required; server prerequisite is the operator steering queue backlog item (for useful message delivery on wakeup).

#### Test Cases

- [ ] T001: Ensure that user can send message via Slack when unprompted by the agent and test that it is stored in the database queue for heartbeat (ping) pickup by the agent.
- [ ] T002: Test that the agent can enter a heartbeat loop pattern where it sends a ping, receives a message from the operator steering queue, processes it, then waits again — ensuring the keep-alive pattern works end-to-end with the new steering queue feature.

